## S&P 500 analysis


```python
import wrds
import pandas as pd
from datetime import datetime, timedelta

# Connect to WRDS
db = wrds.Connection()
# List all available libraries
db.list_libraries()
```

    Loading library list...
    Done





    ['aha_sample',
     'ahasamp',
     'auditsmp',
     'auditsmp_all',
     'bank',
     'bank_all',
     'bank_premium_samp',
     'banksamp',
     'block',
     'block_all',
     'boardex',
     'boardex_eur',
     'boardex_na',
     'boardex_row',
     'boardex_trial',
     'boardex_uk',
     'boardsmp',
     'bvd_amadeus_trial',
     'bvd_bvdbankf_trial',
     'bvd_orbis_trial',
     'bvdsamp',
     'calcbench_trial',
     'calcbnch',
     'cboe',
     'cboe_all',
     'cboe_sample',
     'cboesamp',
     'ciqsamp',
     'ciqsamp_capstrct',
     'ciqsamp_common',
     'ciqsamp_keydev',
     'ciqsamp_pplintel',
     'ciqsamp_ratings',
     'ciqsamp_transactions',
     'ciqsamp_transcripts',
     'cisdmsmp',
     'columnar',
     'comp',
     'comp_bank',
     'comp_bank_daily',
     'comp_global',
     'comp_global_daily',
     'comp_na_annual_all',
     'comp_na_daily_all',
     'comp_na_monthly_all',
     'comp_segments_hist',
     'comp_segments_hist_daily',
     'compa',
     'compb',
     'compg',
     'compm',
     'compsamp',
     'compsamp_all',
     'compsamp_snapshot',
     'compseg',
     'contrib',
     'contrib_as_filed_financials',
     'contrib_char_returns',
     'contrib_corporate_culture',
     'contrib_general',
     'contrib_global_factor',
     'contrib_intangible_value',
     'contrib_kpss',
     'contrib_liva',
     'crsp',
     'crsp_a_ccm',
     'crsp_a_stock',
     'crspsamp',
     'crspsamp_all',
     'crspsamp_mf',
     'csmsamp_all',
     'djones',
     'djones_all',
     'dmef',
     'dmef_all',
     'doe',
     'doe_all',
     'etfg_samp',
     'etfgsamp',
     'factsamp_all',
     'factsamp_revere',
     'ff',
     'ff_all',
     'fisdsamp',
     'fisdsamp_all',
     'fjc',
     'fjc_linking',
     'fjc_litigation',
     'frb',
     'frb_all',
     'fssamp',
     'ftsesamp',
     'ftsesamp_russell_us',
     'gutenberg',
     'hfrsamp',
     'hfrsamp_hfrdb',
     'ibessamp_kpi',
     'ifgrsamp',
     'infogroupsamp_business',
     'infogroupsamp_residential',
     'insdsamp',
     'iri',
     'iri_all',
     'kpisamp',
     'lspd',
     'lspd_daily',
     'lspd_monthly',
     'macrofin',
     'macrofin_comm_trade',
     'midas',
     'morningstarsamp_cisdm',
     'mrktsamp',
     'mrktsamp_cds',
     'mrktsamp_cdx',
     'mrktsamp_msf',
     'msci_esg_samp',
     'msciesmp',
     'msrb',
     'msrb_all',
     'msrbsamp',
     'msrbsamp_all',
     'omtrial',
     'optionmsamp_europe',
     'optionmsamp_us',
     'otc',
     'otc_endofday',
     'phlx',
     'phlx_all',
     'pitchsmp',
     'preqsamp',
     'preqsamp_all',
     'public',
     'public_all',
     'pwt',
     'pwt_all',
     'reprisk_sample',
     'repsamp',
     'revelio_samp',
     'revsamp',
     'risksamp',
     'risksamp_all',
     'rq_all',
     'rstat_samp',
     'rstatsmp',
     'sdcsamp',
     'snapsamp',
     'snlsamp',
     'snlsamp_fig',
     'sustainalyticssamp_all',
     'sustsamp',
     'taqmsamp',
     'taqmsamp_all',
     'taqsamp',
     'taqsamp_all',
     'totalq',
     'totalq_all',
     'tr_sdc_samples',
     'trace',
     'trace_enhanced',
     'trace_standard',
     'trcstsmp',
     'trdbdmismp',
     'trdbwbsmp',
     'trdssamp',
     'tresgsmp',
     'trsamp',
     'trsamp_all',
     'trsamp_db_dmi',
     'trsamp_db_wb',
     'trsamp_ds_eq',
     'trsamp_dscom',
     'trsamp_dsecon',
     'trsamp_dsfut',
     'trsamp_esg',
     'trsamp_sdc_ma',
     'trsamp_sdc_ni',
     'trsamp_worldscope',
     'trucost_samp',
     'twoiq_samp',
     'twoiqsmp',
     'wenvsmp',
     'wmfsmp',
     'wrds_environmental_samp',
     'wrds_insiders_samp',
     'wrds_lib_internal',
     'wrds_mutualfund_samp',
     'wrdsapps',
     'wrdsapps_eushort',
     'wrdsapps_evtstudy_int',
     'wrdsapps_evtstudy_lr',
     'wrdsapps_evtstudy_us',
     'wrdsapps_finratio',
     'wrdsapps_finratio_ccm',
     'wrdsapps_link_comp_eushort',
     'wrdsapps_link_crsp_bond',
     'wrdsapps_link_crsp_comp_bdx',
     'wrdsapps_link_crsp_factset',
     'wrdsapps_link_crsp_taq',
     'wrdsapps_link_supplychain',
     'wrdsapps_patents',
     'wrdsapps_subsidiary',
     'wrdsapps_windices',
     'wrdssec_midas',
     'zacksamp',
     'zacksamp_all']




```python
import wrds
import pandas as pd
from datetime import datetime, timedelta

# Connect to WRDS
db = wrds.Connection()

# Define date range for historical panel
today = datetime.today().date()
start_date = today - timedelta(days=365 * 10)  # Last 10 years
quarterly_dates = pd.date_range(start=start_date, end=today, freq='Q')

# STEP 1: Extract current S&P 500 firms using `curr_sp500_flag`
print("Fetching current S&P 500 firms...")
sp500_current = db.raw_sql("""
    SELECT gvkey, tic AS ticker, curr_sp500_flag
    FROM comp.security
    WHERE curr_sp500_flag = 1
""")

# STEP 2: Get Compustat company names
print("Fetching company names...")
company_info = db.raw_sql("SELECT gvkey, conm FROM comp.company")

# Merge company names
sp500_current = sp500_current.merge(company_info, on="gvkey", how="left")

# STEP 3: Load CRSP-Compustat linking table
print("Loading CRSP-Compustat linking table...")
linktable = db.raw_sql("""
    SELECT gvkey, lpermno AS permno, linkdt, linkenddt, linktype, linkprim
    FROM crsp.ccmxpf_linktable
    WHERE linktype IN ('LU', 'LC') AND linkprim IN ('P', 'C')
""")

# Merge to get CRSP identifiers
sp500_linked = sp500_current.merge(linktable, on="gvkey", how="left")

# STEP 4: Build historical panel using CRSP monthly stock data
print("Fetching CRSP historical price & return data...")
crsp_data = db.raw_sql(f"""
    SELECT date, permno, prc, ret, vol, shrout
    FROM crsp.msf
    WHERE date >= '{start_date}'
""")

# STEP 5: Filter only firms that were active in CRSP at each snapshot date
historical_snapshots = []

for snapshot_date in quarterly_dates:
    print(f"Processing snapshot: {snapshot_date.date()}...")

    # Filter CRSP firms active at this date
    active_firms = crsp_data[crsp_data['date'] == snapshot_date.date()]
    
    # Merge with S&P 500 linked firms
    snapshot = active_firms.merge(sp500_linked, on='permno', how='inner')
    
    # Store snapshot
    snapshot['snapshot_date'] = snapshot_date.date()
    historical_snapshots.append(snapshot[['snapshot_date', 'permno', 'gvkey', 'ticker', 'conm', 'prc', 'ret', 'vol', 'shrout']])

# STEP 6: Combine all snapshots into a single panel
historical_sp500 = pd.concat(historical_snapshots).drop_duplicates()

# Save to CSV
historical_sp500.to_csv("historical_sp500_panel.csv", index=False)
print("âœ… Saved historical S&P 500 panel: historical_sp500_panel.csv")

db.close()
```

    Loading library list...
    Done
    Fetching current S&P 500 firms...
    Fetching company names...
    Loading CRSP-Compustat linking table...


    /tmp/29397410.1.jupyterhub.q/ipykernel_1733811/2342616988.py:11: FutureWarning: 'Q' is deprecated and will be removed in a future version, please use 'QE' instead.
      quarterly_dates = pd.date_range(start=start_date, end=today, freq='Q')


    Fetching CRSP historical price & return data...
    Processing snapshot: 2015-03-31...
    Processing snapshot: 2015-06-30...
    Processing snapshot: 2015-09-30...
    Processing snapshot: 2015-12-31...
    Processing snapshot: 2016-03-31...
    Processing snapshot: 2016-06-30...
    Processing snapshot: 2016-09-30...
    Processing snapshot: 2016-12-31...
    Processing snapshot: 2017-03-31...
    Processing snapshot: 2017-06-30...
    Processing snapshot: 2017-09-30...
    Processing snapshot: 2017-12-31...
    Processing snapshot: 2018-03-31...
    Processing snapshot: 2018-06-30...
    Processing snapshot: 2018-09-30...
    Processing snapshot: 2018-12-31...
    Processing snapshot: 2019-03-31...
    Processing snapshot: 2019-06-30...
    Processing snapshot: 2019-09-30...
    Processing snapshot: 2019-12-31...
    Processing snapshot: 2020-03-31...
    Processing snapshot: 2020-06-30...
    Processing snapshot: 2020-09-30...
    Processing snapshot: 2020-12-31...
    Processing snapshot: 2021-03-31...
    Processing snapshot: 2021-06-30...
    Processing snapshot: 2021-09-30...
    Processing snapshot: 2021-12-31...
    Processing snapshot: 2022-03-31...
    Processing snapshot: 2022-06-30...
    Processing snapshot: 2022-09-30...
    Processing snapshot: 2022-12-31...
    Processing snapshot: 2023-03-31...
    Processing snapshot: 2023-06-30...
    Processing snapshot: 2023-09-30...
    Processing snapshot: 2023-12-31...
    Processing snapshot: 2024-03-31...
    Processing snapshot: 2024-06-30...
    Processing snapshot: 2024-09-30...
    Processing snapshot: 2024-12-31...
    âœ… Saved historical S&P 500 panel: historical_sp500_panel.csv



```python
import wrds
import re

# Connect to WRDS
db = wrds.Connection()

# Function to get all column names for a given table
def get_filtered_columns(library, table, regex):
    columns = db.describe_table(library=library, table=table)['name'].tolist()
    filtered = [col for col in columns if re.search(regex, col, re.IGNORECASE)]
    return filtered

# Define regex patterns
board_size_pattern = r'board|size|num_directors'
gender_pattern = r'female|women|gender'

# Get relevant columns for Board Size & Gender Diversity
board_size_cols = get_filtered_columns('boardex', 'na_board_characteristics', board_size_pattern)
gender_cols = get_filtered_columns('boardex', 'na_wrds_org_composition', gender_pattern)

print("ðŸ” Board Size Columns:", board_size_cols)
print("ðŸ” Gender Diversity Columns:", gender_cols)

db.close()
```

    Loading library list...
    Done
    Approximately 491872 rows in boardex.na_board_characteristics.
    Approximately 2182668 rows in boardex.na_wrds_org_composition.
    ðŸ” Board Size Columns: ['boardname', 'boardid']
    ðŸ” Gender Diversity Columns: []



```python
import wrds
import pandas as pd
import json

# Connect to WRDS
db = wrds.Connection()

# Choose the table and library
library = 'boardex'
table = 'na_wrds_org_composition'  # or 'na_board_characteristics'

# Get column metadata (includes name, type, nullable, comment)
meta = db.describe_table(library=library, table=table)

# Create a name: comment dictionary
col_dict = {
    row['name']: row['comment'] if pd.notnull(row['comment']) else ''
    for _, row in meta.iterrows()
}

# Export to JSON
with open(f"{table}_columns.json", "w") as f:
    json.dump(col_dict, f, indent=4)

print(f"âœ… Saved column metadata to {table}_columns.json")

db.close()
```

    Loading library list...
    Done
    Approximately 2182668 rows in boardex.na_wrds_org_composition.
    âœ… Saved column metadata to na_wrds_org_composition_columns.json



```python
import wrds
import pandas as pd
import json

# Connect to WRDS
db = wrds.Connection()

# Tables of interest
tables = [
    ("boardex", "na_board_characteristics"),
    ("boardex", "na_wrds_org_composition"),
]

for library, table in tables:
    print(f"Processing {library}.{table}...")

    # Get metadata
    meta = db.describe_table(library=library, table=table)

    # Build name: comment dictionary
    col_dict = {
        row['name']: row['comment'] if pd.notnull(row['comment']) else ''
        for _, row in meta.iterrows()
    }

    # Save to JSON
    json_path = f"{table}_columns.json"
    with open(json_path, "w") as f:
        json.dump(col_dict, f, indent=4)

    print(f"âœ… Saved column metadata to {json_path}")

# Close WRDS connection
db.close()
```

    Loading library list...
    Done
    Processing boardex.na_board_characteristics...
    Approximately 491872 rows in boardex.na_board_characteristics.
    âœ… Saved column metadata to na_board_characteristics_columns.json
    Processing boardex.na_wrds_org_composition...
    Approximately 2182668 rows in boardex.na_wrds_org_composition.
    âœ… Saved column metadata to na_wrds_org_composition_columns.json



```python
import wrds
import pandas as pd

# Connect to WRDS
db = wrds.Connection()

# Step 1: Load S&P 500 panel (ensure it has the right identifier)
sp500_panel = pd.read_csv("historical_sp500_panel.csv")

# Step 2: Fetch BoardEx data using correct column names
print("Fetching BoardEx board composition data...")
boardex_data = db.raw_sql("""
    SELECT boardid, annualreportdate, numberdirectors, genderratio
    FROM boardex.na_board_characteristics
    WHERE numberdirectors IS NOT NULL AND genderratio IS NOT NULL
""")

# Step 3: Convert `annualreportdate` to `fiscal_year`
boardex_data["fiscal_year"] = pd.to_datetime(boardex_data["annualreportdate"]).dt.year

# Step 4: Compute gender diversity metrics
boardex_data["pct_female_directors"] = 1 - boardex_data["genderratio"]
boardex_data["estimated_num_female_directors"] = (
    boardex_data["numberdirectors"] * boardex_data["pct_female_directors"]
).round(2)

# Step 5: Merge BoardEx data into S&P 500 panel using `boardid`
print("Merging BoardEx data with S&P 500 panel...")
sp500_enriched = sp500_panel.merge(
    boardex_data,
    left_on=["boardid", "fiscal_year"],  # Adjust if S&P 500 uses another ID
    right_on=["boardid", "fiscal_year"],
    how="left"
)

# Step 6: Save final enriched dataset
output_file = "sp500_panel_with_board_gender.csv"
sp500_enriched.to_csv(output_file, index=False)
print(f"âœ… Enriched S&P 500 panel saved to: {output_file}")

# Close WRDS connection
db.close()
```

    Loading library list...
    Done
    Fetching BoardEx board composition data...



    ---------------------------------------------------------------------------

    OverflowError                             Traceback (most recent call last)

    File np_datetime.pyx:319, in pandas._libs.tslibs.np_datetime.pydate_to_dt64()


    OverflowError: Overflow occurred in npy_datetimestruct_to_datetime

    
    The above exception was the direct cause of the following exception:


    OutOfBoundsDatetime                       Traceback (most recent call last)

    Cell In[25], line 19
         12 boardex_data = db.raw_sql("""
         13     SELECT boardid, annualreportdate, numberdirectors, genderratio
         14     FROM boardex.na_board_characteristics
         15     WHERE numberdirectors IS NOT NULL AND genderratio IS NOT NULL
         16 """)
         18 # Step 3: Convert `annualreportdate` to `fiscal_year`
    ---> 19 boardex_data["fiscal_year"] = pd.to_datetime(boardex_data["annualreportdate"]).dt.year
         21 # Step 4: Compute gender diversity metrics
         22 boardex_data["pct_female_directors"] = 1 - boardex_data["genderratio"]


    File /usr/local/sas/jupyterhub/prod/venvs/20240522/lib/python3.11/site-packages/pandas/core/tools/datetimes.py:1063, in to_datetime(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)
       1061             result = arg.tz_localize("utc")
       1062 elif isinstance(arg, ABCSeries):
    -> 1063     cache_array = _maybe_cache(arg, format, cache, convert_listlike)
       1064     if not cache_array.empty:
       1065         result = arg.map(cache_array)


    File /usr/local/sas/jupyterhub/prod/venvs/20240522/lib/python3.11/site-packages/pandas/core/tools/datetimes.py:247, in _maybe_cache(arg, format, cache, convert_listlike)
        245 unique_dates = unique(arg)
        246 if len(unique_dates) < len(arg):
    --> 247     cache_dates = convert_listlike(unique_dates, format)
        248     # GH#45319
        249     try:


    File /usr/local/sas/jupyterhub/prod/venvs/20240522/lib/python3.11/site-packages/pandas/core/tools/datetimes.py:435, in _convert_listlike_datetimes(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)
        432 if format is not None and format != "mixed":
        433     return _array_strptime_with_fallback(arg, name, utc, format, exact, errors)
    --> 435 result, tz_parsed = objects_to_datetime64(
        436     arg,
        437     dayfirst=dayfirst,
        438     yearfirst=yearfirst,
        439     utc=utc,
        440     errors=errors,
        441     allow_object=True,
        442 )
        444 if tz_parsed is not None:
        445     # We can take a shortcut since the datetime64 numpy array
        446     # is in UTC
        447     out_unit = np.datetime_data(result.dtype)[0]


    File /usr/local/sas/jupyterhub/prod/venvs/20240522/lib/python3.11/site-packages/pandas/core/arrays/datetimes.py:2398, in objects_to_datetime64(data, dayfirst, yearfirst, utc, errors, allow_object, out_unit)
       2395 # if str-dtype, convert
       2396 data = np.asarray(data, dtype=np.object_)
    -> 2398 result, tz_parsed = tslib.array_to_datetime(
       2399     data,
       2400     errors=errors,
       2401     utc=utc,
       2402     dayfirst=dayfirst,
       2403     yearfirst=yearfirst,
       2404     creso=abbrev_to_npy_unit(out_unit),
       2405 )
       2407 if tz_parsed is not None:
       2408     # We can take a shortcut since the datetime64 numpy array
       2409     #  is in UTC
       2410     return result, tz_parsed


    File tslib.pyx:414, in pandas._libs.tslib.array_to_datetime()


    File tslib.pyx:596, in pandas._libs.tslib.array_to_datetime()


    File tslib.pyx:512, in pandas._libs.tslib.array_to_datetime()


    File np_datetime.pyx:322, in pandas._libs.tslibs.np_datetime.pydate_to_dt64()


    OutOfBoundsDatetime: Out of bounds nanosecond timestamp: 9000-01-01, at position 4



```python
import wrds

# Connect to WRDS
db = wrds.Connection()

# Check column names for na_board_characteristics
boardex_cols = db.describe_table(library="boardex", table="na_board_characteristics")
print(boardex_cols["name"].tolist())  # Print column names

db.close()
```

    Loading library list...
    Done
    Approximately 491872 rows in boardex.na_board_characteristics.
    ['boardname', 'rowtype', 'attrition', 'attrition3yr', 'genderratio', 'boardid', 'timeretirement', 'timerole', 'timebrd', 'timeinco', 'avgtimeothco', 'totnolstdbrd', 'totnounlstdbrd', 'totnoothlstdbrd', 'totcurrnolstdbrd', 'totcurrnounlstdbrd', 'totcurrnoothlstdbrd', 'noquals', 'succession', 'nationalitymix', 'numberdirectors', 'stdevtimebrd', 'stdevtimeinco', 'stdevtotnolstdbrd', 'stdevtotcurrnolstdbrd', 'stdevnoquals', 'stdevage', 'annualreportdate']



```python
# Check for extreme date values
print(boardex_data["annualreportdate"].describe())

# Find any out-of-bounds dates
invalid_dates = boardex_data[
    (boardex_data["annualreportdate"] < "1900-01-01") |
    (boardex_data["annualreportdate"] > "2100-01-01")
]
print(invalid_dates)
```

    count         487178
    unique           319
    top       9000-01-01
    freq           27983
    Name: annualreportdate, dtype: object



    ---------------------------------------------------------------------------

    TypeError                                 Traceback (most recent call last)

    Cell In[26], line 6
          2 print(boardex_data["annualreportdate"].describe())
          4 # Find any out-of-bounds dates
          5 invalid_dates = boardex_data[
    ----> 6     (boardex_data["annualreportdate"] < "1900-01-01") |
          7     (boardex_data["annualreportdate"] > "2100-01-01")
          8 ]
          9 print(invalid_dates)


    File /usr/local/sas/jupyterhub/prod/venvs/20240522/lib/python3.11/site-packages/pandas/core/ops/common.py:76, in _unpack_zerodim_and_defer.<locals>.new_method(self, other)
         72             return NotImplemented
         74 other = item_from_zerodim(other)
    ---> 76 return method(self, other)


    File /usr/local/sas/jupyterhub/prod/venvs/20240522/lib/python3.11/site-packages/pandas/core/arraylike.py:48, in OpsMixin.__lt__(self, other)
         46 @unpack_zerodim_and_defer("__lt__")
         47 def __lt__(self, other):
    ---> 48     return self._cmp_method(other, operator.lt)


    File /usr/local/sas/jupyterhub/prod/venvs/20240522/lib/python3.11/site-packages/pandas/core/series.py:6119, in Series._cmp_method(self, other, op)
       6116 lvalues = self._values
       6117 rvalues = extract_array(other, extract_numpy=True, extract_range=True)
    -> 6119 res_values = ops.comparison_op(lvalues, rvalues, op)
       6121 return self._construct_result(res_values, name=res_name)


    File /usr/local/sas/jupyterhub/prod/venvs/20240522/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:344, in comparison_op(left, right, op)
        341     return invalid_comparison(lvalues, rvalues, op)
        343 elif lvalues.dtype == object or isinstance(rvalues, str):
    --> 344     res_values = comp_method_OBJECT_ARRAY(op, lvalues, rvalues)
        346 else:
        347     res_values = _na_arithmetic_op(lvalues, rvalues, op, is_cmp=True)


    File /usr/local/sas/jupyterhub/prod/venvs/20240522/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:129, in comp_method_OBJECT_ARRAY(op, x, y)
        127     result = libops.vec_compare(x.ravel(), y.ravel(), op)
        128 else:
    --> 129     result = libops.scalar_compare(x.ravel(), y, op)
        130 return result.reshape(x.shape)


    File ops.pyx:107, in pandas._libs.ops.scalar_compare()


    TypeError: '<' not supported between instances of 'datetime.date' and 'str'



```python
# Remove placeholder dates (9000-01-01)
boardex_data = boardex_data[boardex_data["annualreportdate"] != "9000-01-01"]

# Convert remaining dates safely
boardex_data["annualreportdate"] = pd.to_datetime(boardex_data["annualreportdate"], errors="coerce")

# Extract fiscal year
boardex_data["fiscal_year"] = boardex_data["annualreportdate"].dt.year

# Check output
print(boardex_data["fiscal_year"].describe())
```

    count    459195.000000
    mean       2013.509370
    std           6.517208
    min        1997.000000
    25%        2008.000000
    50%        2014.000000
    75%        2019.000000
    max        2025.000000
    Name: fiscal_year, dtype: float64



```python
import pandas as pd

# Step 1: Load your historical S&P 500 panel
sp500_panel = pd.read_csv("historical_sp500_panel.csv")

# Step 2: Ensure boardid and fiscal_year exist in both datasets
print("Checking columns in both datasets...")
print("S&P 500 Panel Columns:", sp500_panel.columns)
print("BoardEx Data Columns:", boardex_data.columns)

# Step 3: Merge BoardEx data into S&P 500 panel
print("Merging BoardEx data with S&P 500 panel...")
sp500_enriched = sp500_panel.merge(
    boardex_data,
    on=["boardid", "fiscal_year"],  # Make sure these exist in both datasets
    how="left"
)

# Step 4: Save the final enriched dataset
output_file = "sp500_panel_with_board_gender.csv"
sp500_enriched.to_csv(output_file, index=False)

print(f"âœ… Enriched S&P 500 panel saved to: {output_file}")
```

    Checking columns in both datasets...
    S&P 500 Panel Columns: Index(['snapshot_date', 'permno', 'gvkey', 'ticker', 'conm', 'prc', 'ret',
           'vol', 'shrout'],
          dtype='object')
    BoardEx Data Columns: Index(['boardid', 'annualreportdate', 'numberdirectors', 'genderratio',
           'fiscal_year'],
          dtype='object')
    Merging BoardEx data with S&P 500 panel...



    ---------------------------------------------------------------------------

    KeyError                                  Traceback (most recent call last)

    /tmp/29397410.1.jupyterhub.q/ipykernel_1733811/3861048979.py in ?()
          9 print("BoardEx Data Columns:", boardex_data.columns)
         10 
         11 # Step 3: Merge BoardEx data into S&P 500 panel
         12 print("Merging BoardEx data with S&P 500 panel...")
    ---> 13 sp500_enriched = sp500_panel.merge(
         14     boardex_data,
         15     on=["boardid", "fiscal_year"],  # Make sure these exist in both datasets
         16     how="left"


    /usr/local/sas/jupyterhub/prod/venvs/20240522/lib/python3.11/site-packages/pandas/core/frame.py in ?(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)
      10828         validate: MergeValidate | None = None,
      10829     ) -> DataFrame:
      10830         from pandas.core.reshape.merge import merge
      10831 
    > 10832         return merge(
      10833             self,
      10834             right,
      10835             how=how,


    /usr/local/sas/jupyterhub/prod/venvs/20240522/lib/python3.11/site-packages/pandas/core/reshape/merge.py in ?(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)
        166             validate=validate,
        167             copy=copy,
        168         )
        169     else:
    --> 170         op = _MergeOperation(
        171             left_df,
        172             right_df,
        173             how=how,


    /usr/local/sas/jupyterhub/prod/venvs/20240522/lib/python3.11/site-packages/pandas/core/reshape/merge.py in ?(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)
        790             self.right_join_keys,
        791             self.join_names,
        792             left_drop,
        793             right_drop,
    --> 794         ) = self._get_merge_keys()
        795 
        796         if left_drop:
        797             self.left = self.left._drop_labels_or_levels(left_drop)


    /usr/local/sas/jupyterhub/prod/venvs/20240522/lib/python3.11/site-packages/pandas/core/reshape/merge.py in ?(self)
       1306                     if lk is not None:
       1307                         # Then we're either Hashable or a wrong-length arraylike,
       1308                         #  the latter of which will raise
       1309                         lk = cast(Hashable, lk)
    -> 1310                         left_keys.append(left._get_label_or_level_values(lk))
       1311                         join_names.append(lk)
       1312                     else:
       1313                         # work-around for merge_asof(left_index=True)


    /usr/local/sas/jupyterhub/prod/venvs/20240522/lib/python3.11/site-packages/pandas/core/generic.py in ?(self, key, axis)
       1907             values = self.xs(key, axis=other_axes[0])._values
       1908         elif self._is_level_reference(key, axis=axis):
       1909             values = self.axes[axis].get_level_values(key)._values
       1910         else:
    -> 1911             raise KeyError(key)
       1912 
       1913         # Check for duplicates
       1914         if values.ndim > 1:


    KeyError: 'boardid'



```python
boardex_data['boardid'].describe()
```




    count    4.871780e+05
    mean     8.679568e+05
    std      1.040966e+06
    min      3.000000e+00
    25%      1.927000e+04
    50%      4.686280e+05
    75%      1.660189e+06
    max      3.934622e+06
    Name: boardid, dtype: float64




```python
import pandas as pd

# Load S&P 500 panel
sp500_panel = pd.read_csv("historical_sp500_panel.csv")

# Load BoardEx company profile (to get tickers)
boardex_ids = db.raw_sql("""
    SELECT boardid, ticker, isin, boardname
    FROM boardex.na_wrds_company_profile
""")

# Convert tickers to uppercase for safe merging
sp500_panel["ticker"] = sp500_panel["ticker"].str.upper()
boardex_ids["ticker"] = boardex_ids["ticker"].str.upper()

# Check how many tickers match
matches = sp500_panel["ticker"].isin(boardex_ids["ticker"]).sum()
total_sp500 = len(sp500_panel)
print(f"âœ… Found {matches} ticker matches out of {total_sp500} S&P 500 companies.")

# If matches are good, merge on ticker
if matches > 0:
    sp500_panel = sp500_panel.merge(boardex_ids, on="ticker", how="left")
    print("âœ… Merged using ticker.")
else:
    print("âš ï¸ Few or no matches. Consider using ISIN or fuzzy name matching.")
```

    âœ… Found 13276 ticker matches out of 13276 S&P 500 companies.
    âœ… Merged using ticker.



```python
# Create fiscal year in sp500 data
sp500_panel["snapshot_date"] = pd.to_datetime(sp500_panel["snapshot_date"], errors="coerce")
sp500_panel["fiscal_year"] = sp500_panel["snapshot_date"].dt.year

#  Merge BoardEx metrics into the enriched S&P 500 panel
sp500_enriched = sp500_panel.merge(
    boardex_data,
    on=["boardid", "fiscal_year"],
    how="left"
)

# Save the final result
sp500_enriched.to_csv("sp500_panel_with_board_gender.csv", index=False)
print("âœ… Final panel with board size and gender diversity saved!")
```

    âœ… Final panel with board size and gender diversity saved!



```python
# Check % coverage for BoardEx fields
coverage = sp500_enriched["numberdirectors"].notna().mean()
print(f"BoardEx coverage: {coverage:.1%} of S&P 500 firm-years")

# Spot check example
sp500_enriched[["ticker", "fiscal_year", "numberdirectors", "pct_female_directors"]].head()
```

    BoardEx coverage: 95.2% of S&P 500 firm-years



    ---------------------------------------------------------------------------

    KeyError                                  Traceback (most recent call last)

    Cell In[39], line 6
          3 print(f"BoardEx coverage: {coverage:.1%} of S&P 500 firm-years")
          5 # Spot check example
    ----> 6 sp500_enriched[["ticker", "fiscal_year", "numberdirectors", "pct_female_directors"]].head()


    File /usr/local/sas/jupyterhub/prod/venvs/20240522/lib/python3.11/site-packages/pandas/core/frame.py:4108, in DataFrame.__getitem__(self, key)
       4106     if is_iterator(key):
       4107         key = list(key)
    -> 4108     indexer = self.columns._get_indexer_strict(key, "columns")[1]
       4110 # take() does not accept boolean indexers
       4111 if getattr(indexer, "dtype", None) == bool:


    File /usr/local/sas/jupyterhub/prod/venvs/20240522/lib/python3.11/site-packages/pandas/core/indexes/base.py:6200, in Index._get_indexer_strict(self, key, axis_name)
       6197 else:
       6198     keyarr, indexer, new_indexer = self._reindex_non_unique(keyarr)
    -> 6200 self._raise_if_missing(keyarr, indexer, axis_name)
       6202 keyarr = self.take(indexer)
       6203 if isinstance(key, Index):
       6204     # GH 42790 - Preserve name from an Index


    File /usr/local/sas/jupyterhub/prod/venvs/20240522/lib/python3.11/site-packages/pandas/core/indexes/base.py:6252, in Index._raise_if_missing(self, key, indexer, axis_name)
       6249     raise KeyError(f"None of [{key}] are in the [{axis_name}]")
       6251 not_found = list(ensure_index(key)[missing_mask.nonzero()[0]].unique())
    -> 6252 raise KeyError(f"{not_found} not in index")


    KeyError: "['pct_female_directors'] not in index"



```python

```
